{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1 : What is Information Gain, and how is it used in Decision Trees?**"
      ],
      "metadata": {
        "id": "edAsW_5j9-c8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introduction\n",
        "\n",
        "Information Gain is a measure used in Decision Tree algorithms to decide which feature should be selected as a node in the tree. It tells us how much information a feature provides about the class label by reducing uncertainty in the dataset.\n",
        "\n",
        "Entropy\n",
        "\n",
        "Entropy is a measure of randomness or impurity in a dataset.\n",
        "\n",
        "If all data belongs to one class, entropy is 0\n",
        "\n",
        "If data is evenly split among classes, entropy is high\n",
        "\n",
        "Entropy formula (simplified):\n",
        "Entropy of S = minus (p1 * log2(p1) + p2 * log2(p2) + ...)\n",
        "\n",
        "Where p1, p2, etc. are probabilities of different classes.\n",
        "\n",
        "Information Gain\n",
        "\n",
        "Information Gain measures the reduction in entropy after splitting the dataset using a particular feature.\n",
        "\n",
        "Information Gain formula (simplified):\n",
        "Information Gain = Entropy before split − Entropy after split\n",
        "\n",
        "More clearly:\n",
        "Information Gain of feature A =\n",
        "Entropy of original dataset −\n",
        "(Weighted entropy of subsets created by feature A)\n",
        "\n",
        "Use of Information Gain in Decision Trees\n",
        "\n",
        "Information Gain is used during the construction of a Decision Tree as follows:\n",
        "\n",
        "Calculate entropy of the entire dataset\n",
        "\n",
        "Calculate entropy for each feature after splitting the data\n",
        "\n",
        "Compute Information Gain for each feature\n",
        "\n",
        "Select the feature with the highest Information Gain as the decision node\n",
        "\n",
        "Repeat the process recursively for child nodes\n",
        "\n",
        "This process continues until all data is classified or stopping conditions are met.\n",
        "\n",
        "Example\n",
        "\n",
        "Consider a dataset used to predict whether a person will play a game or not.\n",
        "\n",
        "Initial entropy of dataset = 0.94\n",
        "\n",
        "Information Gain values:\n",
        "\n",
        "Outlook = 0.24\n",
        "\n",
        "Humidity = 0.15\n",
        "\n",
        "Wind = 0.05\n",
        "\n",
        "Since Outlook has the highest Information Gain, it is chosen as the root node of the Decision Tree.\n",
        "\n",
        "Advantages of Information Gain\n",
        "\n",
        "Helps in selecting the most relevant feature\n",
        "\n",
        "Reduces uncertainty in decision making\n",
        "\n",
        "Produces efficient and accurate trees\n",
        "\n",
        "Limitation\n",
        "\n",
        "Information Gain favors attributes with many distinct values, which may lead to overfitting. To overcome this issue, algorithms like C4.5 use Gain Ratio instead.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "Information Gain is a key concept in Decision Trees that helps in choosing the best feature by measuring how much uncertainty is reduced after a split. It plays an important role in building accurate and efficient classification models."
      ],
      "metadata": {
        "id": "wV7ivxsULIxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "Hint: Directly compares the two main impurity measures, highlighting strengths,\n",
        "weaknesses, and appropriate use cases.**\n"
      ],
      "metadata": {
        "id": "pi5QH0tm-En2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Gini Impurity and Entropy are both measures used in Decision Trees to calculate how impure or mixed a dataset is. They help the algorithm decide which feature should be used to split the data at each node. Even though they are used for the same purpose, they work in slightly different ways.\n",
        "\n",
        "\n",
        "\n",
        "### Entropy\n",
        "\n",
        "Entropy measures the **amount of uncertainty or randomness** in the data.\n",
        "\n",
        "* Entropy is 0 when all data belongs to one class\n",
        "* Entropy is high when data is evenly split between classes\n",
        "\n",
        "Entropy focuses more on how evenly the data is distributed among different classes. It is commonly used in the ID3 algorithm.\n",
        "\n",
        "\n",
        "\n",
        "### Gini Impurity\n",
        "\n",
        "Gini Impurity measures **how often a randomly chosen data point would be incorrectly classified** if it was labeled randomly based on the class distribution.\n",
        "\n",
        "* Gini Impurity is 0 when all data belongs to one class\n",
        "* Gini Impurity increases as the data becomes more mixed\n",
        "\n",
        "Gini is commonly used in CART (Classification and Regression Tree) algorithms.\n",
        "\n",
        "\n",
        "### Key Differences Between Gini Impurity and Entropy\n",
        "\n",
        "1. **Calculation Method**\n",
        "\n",
        "   * Entropy uses logarithmic calculations and is more complex\n",
        "   * Gini Impurity uses simple probability calculations and is easier to compute\n",
        "\n",
        "2. **Speed**\n",
        "\n",
        "   * Entropy is slower because of log calculations\n",
        "   * Gini Impurity is faster and works well for large datasets\n",
        "\n",
        "3. **Sensitivity**\n",
        "\n",
        "   * Entropy is more sensitive to small changes in probabilities\n",
        "   * Gini is less sensitive and gives smoother splits\n",
        "\n",
        "4. **Result Quality**\n",
        "\n",
        "   * Entropy often produces slightly more balanced trees\n",
        "   * Gini may create simpler trees with similar accuracy\n",
        "\n",
        "\n",
        "\n",
        "### Advantages\n",
        "\n",
        "**Entropy:**\n",
        "\n",
        "* Gives detailed information about data distribution\n",
        "* Useful when precise splits are required\n",
        "\n",
        "**Gini Impurity:**\n",
        "\n",
        "* Faster and computationally efficient\n",
        "* Works well for large datasets\n",
        "\n",
        "\n",
        "\n",
        "### Disadvantages\n",
        "\n",
        "**Entropy:**\n",
        "\n",
        "* Computationally expensive\n",
        "* Slower compared to Gini\n",
        "\n",
        "**Gini Impurity:**\n",
        "\n",
        "* Slightly less informative in some cases\n",
        "* Can favor dominant classes\n",
        "\n",
        "\n",
        "\n",
        "### Use Cases\n",
        "\n",
        "* Entropy is preferred when understanding uncertainty is important\n",
        "* Gini Impurity is preferred when speed and efficiency are required\n",
        "* In practice, both often give similar results\n",
        "\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Both Gini Impurity and Entropy are impurity measures used in Decision Trees to choose the best feature for splitting data. Entropy focuses on uncertainty, while Gini Impurity focuses on misclassification probability."
      ],
      "metadata": {
        "id": "qA-LmlG1LuST"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "76vIc8qY_JKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2xdJL-3r-rHV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3:What is Pre-Pruning in Decision Trees?**"
      ],
      "metadata": {
        "id": "wA8szRbC-ISq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Pre-Pruning is a technique used in Decision Trees to **stop the tree from growing too large**. In this method, the tree construction is stopped early, before it fully fits the training data. The main purpose of pre-pruning is to **avoid overfitting**.\n",
        "\n",
        "Overfitting happens when a decision tree learns too much detail from the training data and performs poorly on new or unseen data.\n",
        "\n",
        "\n",
        "\n",
        "### Why Pre-Pruning is Needed\n",
        "\n",
        "Decision Trees can grow very deep and complex if there are no limits. This makes the model:\n",
        "\n",
        "* Hard to understand\n",
        "* Too specific to training data\n",
        "* Less accurate on test data\n",
        "\n",
        "Pre-pruning controls this problem by limiting the growth of the tree.\n",
        "\n",
        "\n",
        "\n",
        "### How Pre-Pruning Works\n",
        "\n",
        "In pre-pruning, certain conditions are checked **before making a split**. If the conditions are not satisfied, the split is not performed.\n",
        "\n",
        "Common pre-pruning conditions include:\n",
        "\n",
        "1. Minimum number of samples required to split a node\n",
        "2. Maximum depth of the tree\n",
        "3. Minimum Information Gain or Gini reduction required\n",
        "4. Maximum number of leaf nodes\n",
        "\n",
        "If a condition fails, the node becomes a leaf node.\n",
        "\n",
        "\n",
        "\n",
        "### Example\n",
        "\n",
        "Suppose a Decision Tree is being built for classification. At a certain node, splitting the data gives very little improvement in accuracy. With pre-pruning, the algorithm will **stop splitting at that point** and treat the node as a final decision.\n",
        "\n",
        "\n",
        "\n",
        "### Advantages of Pre-Pruning\n",
        "\n",
        "* Prevents overfitting\n",
        "* Reduces complexity of the tree\n",
        "* Improves generalization to new data\n",
        "* Faster training time\n",
        "\n",
        "\n",
        "\n",
        "### Disadvantages of Pre-Pruning\n",
        "\n",
        "* Can stop the tree too early\n",
        "* Important patterns in data may be missed\n",
        "* Requires proper selection of stopping conditions\n",
        "\n",
        "\n",
        "\n",
        "### Comparison with Post-Pruning\n",
        "\n",
        "* Pre-pruning stops the tree while it is being built\n",
        "* Post-pruning allows the full tree to grow and then removes unnecessary branches\n",
        "\n",
        "\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Pre-Pruning is an important method in Decision Trees that helps control overfitting by stopping the growth of the tree early. By setting limits such as maximum depth or minimum improvement, it helps create simpler and more reliable models.\n"
      ],
      "metadata": {
        "id": "UbUh-6aoMQ8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4:Write a Python program to train a Decision Tree Classifier using Gini\n",
        "Impurity as the criterion and print the feature importances (practical).\n",
        "Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_.\n",
        "(Include your Python code and output in the code box below.)**\n"
      ],
      "metadata": {
        "id": "xTuE0KSJ-Njf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Create Decision Tree Classifier using Gini Impurity\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(data.feature_names, model.feature_importances_):\n",
        "    print(feature, \":\", importance)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pJwQK8wMutv",
        "outputId": "3200727f-a471-4b5e-ce90-88c5b82b02a3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm) : 0.013333333333333329\n",
            "sepal width (cm) : 0.0\n",
            "petal length (cm) : 0.5640559581320451\n",
            "petal width (cm) : 0.4226107085346215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: What is a Support Vector Machine (SVM)?**"
      ],
      "metadata": {
        "id": "NIpppvNI-Rgt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "A Support Vector Machine (SVM) is a **supervised machine learning algorithm** used mainly for **classification** and sometimes for **regression**. It works by finding the **best boundary** that separates data points of different classes.\n",
        "\n",
        "This boundary is called a **decision boundary** or **hyperplane**.\n",
        "\n",
        "\n",
        "\n",
        "### Basic Idea of SVM\n",
        "\n",
        "The main goal of SVM is to separate data points of different classes in such a way that the distance between the boundary and the nearest data points is as large as possible.\n",
        "\n",
        "These nearest data points are called **support vectors**, and they play an important role in defining the position of the boundary.\n",
        "\n",
        "\n",
        "\n",
        "### How SVM Works\n",
        "\n",
        "1. SVM tries to draw a line (in 2D) or a plane (in higher dimensions) that separates classes\n",
        "2. It chooses the boundary that has the **maximum margin**\n",
        "3. Only the support vectors affect the final boundary\n",
        "4. Other points do not change the decision boundary\n",
        "\n",
        "\n",
        "\n",
        "### Linear and Non-Linear SVM\n",
        "\n",
        "* **Linear SVM** is used when data can be separated using a straight line\n",
        "* **Non-Linear SVM** is used when data is not linearly separable\n",
        "\n",
        "For non-linear cases, SVM uses the **kernel trick** to transform data into higher dimensions where separation becomes possible.\n",
        "\n",
        "\n",
        "\n",
        "### Common Kernels Used in SVM\n",
        "\n",
        "* Linear kernel\n",
        "* Polynomial kernel\n",
        "* Radial Basis Function (RBF) kernel\n",
        "\n",
        "\n",
        "\n",
        "### Advantages of SVM\n",
        "\n",
        "* Works well for high-dimensional data\n",
        "* Effective when number of features is large\n",
        "* Uses only important data points (support vectors)\n",
        "* Provides good accuracy\n",
        "\n",
        "\n",
        "\n",
        "### Disadvantages of SVM\n",
        "\n",
        "* Not suitable for very large datasets\n",
        "* Kernel selection can be difficult\n",
        "* Less interpretable compared to Decision Trees\n",
        "\n",
        "\n",
        "\n",
        "### Applications of SVM\n",
        "\n",
        "* Image classification\n",
        "* Text classification\n",
        "* Face recognition\n",
        "* Bioinformatics\n",
        "\n",
        "\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Support Vector Machine is a powerful machine learning algorithm that finds the best possible boundary to separate data points of different classes. By focusing on support vectors and maximizing margin, SVM provides accurate and reliable classification results.\n",
        "\n"
      ],
      "metadata": {
        "id": "d9OPqP8DNEIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6: What is the Kernel Trick in SVM?**\n"
      ],
      "metadata": {
        "id": "yUmoXxbA-YbX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The Kernel Trick is a technique used in **Support Vector Machines (SVM)** to handle data that **cannot be separated using a straight line**. It helps SVM solve **non-linear classification problems** by transforming the data into a higher-dimensional space.\n",
        "\n",
        "The main idea is to make the data **linearly separable** without actually calculating the transformation explicitly.\n",
        "\n",
        "\n",
        "### Why the Kernel Trick is Needed\n",
        "\n",
        "In many real-world problems, data points are mixed in such a way that a straight line cannot separate them correctly. A simple linear SVM fails in such cases.\n",
        "\n",
        "The Kernel Trick allows SVM to create a **non-linear decision boundary** by mapping the data into a higher dimension where separation becomes possible.\n",
        "\n",
        "\n",
        "\n",
        "### How the Kernel Trick Works\n",
        "\n",
        "Instead of directly transforming data into a higher dimension, the Kernel Trick:\n",
        "\n",
        "1. Computes similarity between data points\n",
        "2. Avoids complex calculations\n",
        "3. Saves computation time and memory\n",
        "\n",
        "This makes SVM efficient even for high-dimensional data.\n",
        "\n",
        "\n",
        "\n",
        "### Common Types of Kernels\n",
        "\n",
        "Some commonly used kernels in SVM are:\n",
        "\n",
        "* **Linear Kernel** – used when data is linearly separable\n",
        "* **Polynomial Kernel** – used for curved decision boundaries\n",
        "* **RBF (Radial Basis Function) Kernel** – widely used for complex patterns\n",
        "\n",
        "\n",
        "\n",
        "### Advantages of the Kernel Trick\n",
        "\n",
        "* Allows SVM to solve non-linear problems\n",
        "* Avoids explicit high-dimensional computation\n",
        "* Improves classification accuracy\n",
        "* Works well for complex datasets\n",
        "\n",
        "\n",
        "\n",
        "### Limitations\n",
        "\n",
        "* Choosing the right kernel can be difficult\n",
        "* Kernel parameters need tuning\n",
        "* Can be slow for very large datasets\n",
        "\n",
        "\n",
        "\n",
        "### Example\n",
        "\n",
        "In image or text classification, data is often not linearly separable. The Kernel Trick helps SVM draw complex boundaries that separate classes effectively.\n",
        "\n",
        "\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "The Kernel Trick is an important concept in SVM that allows it to handle non-linear data by transforming it into a higher-dimensional space in an efficient way. It makes SVM powerful and flexible for real-world problems.\n",
        "\n"
      ],
      "metadata": {
        "id": "4ylTTdUaPGpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "kernels on the Wine dataset, then compare their accuracies.\n",
        "Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting\n",
        "on the same dataset.\n",
        "(Include your Python code and output in the code box below.)**"
      ],
      "metadata": {
        "id": "cAZ-rcNA-aqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train SVM with Linear kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "\n",
        "# Train SVM with RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# Calculate accuracies\n",
        "linear_accuracy = accuracy_score(y_test, y_pred_linear)\n",
        "rbf_accuracy = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print results\n",
        "print(\"Linear Kernel SVM Accuracy:\", linear_accuracy)\n",
        "print(\"RBF Kernel SVM Accuracy:\", rbf_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWk7JYfIOsLd",
        "outputId": "dfa63ee4-16df-4f72-bd7d-131caeeb457b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel SVM Accuracy: 0.9814814814814815\n",
            "RBF Kernel SVM Accuracy: 0.7592592592592593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?**"
      ],
      "metadata": {
        "id": "ZvUikaQL-e6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Naïve Bayes is a **supervised machine learning classifier** based on probability and Bayes’ theorem. It is mainly used for **classification problems**, especially in text classification, spam detection, and medical diagnosis.\n",
        "\n",
        "The classifier predicts the class of a data point by calculating the probability of each class and choosing the one with the highest probability.\n",
        "\n",
        "\n",
        "\n",
        "### Basic Idea of Naïve Bayes\n",
        "\n",
        "Naïve Bayes works on the idea that the **features are independent of each other**. This means it assumes that one feature does not affect another feature while predicting the class.\n",
        "\n",
        "Even though this assumption is not always true in real-world data, the algorithm still performs well in many cases.\n",
        "\n",
        "\n",
        "\n",
        "### Why is it called “Naïve”?\n",
        "\n",
        "It is called “Naïve” because of its **strong and unrealistic assumption** that all features are independent.\n",
        "\n",
        "For example, in text classification, the algorithm assumes that the presence of one word in a document does not affect the presence of another word. This assumption is usually not true, but it simplifies calculations and makes the algorithm fast.\n",
        "\n",
        "\n",
        "\n",
        "### How Naïve Bayes Works\n",
        "\n",
        "1. It calculates the probability of each class\n",
        "2. It calculates the probability of each feature given a class\n",
        "3. It combines these probabilities\n",
        "4. The class with the highest final probability is chosen as the prediction\n",
        "\n",
        "\n",
        "\n",
        "### Advantages of Naïve Bayes\n",
        "\n",
        "* Simple and easy to understand\n",
        "* Very fast and efficient\n",
        "* Works well with large datasets\n",
        "* Performs well in text classification problems\n",
        "\n",
        "\n",
        "\n",
        "### Disadvantages of Naïve Bayes\n",
        "\n",
        "* Assumes feature independence, which is often unrealistic\n",
        "* Performance depends on quality of data\n",
        "* Not suitable for complex relationships\n",
        "\n",
        "\n",
        "\n",
        "### Applications of Naïve Bayes\n",
        "\n",
        "* Spam email detection\n",
        "* Sentiment analysis\n",
        "* Document classification\n",
        "* Medical diagnosis\n",
        "\n",
        "\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Naïve Bayes is a simple yet powerful classification algorithm that uses probability to make predictions. It is called “Naïve” because it assumes that all features are independent, an assumption that simplifies the model but still allows it to perform well in many practical applications.\n"
      ],
      "metadata": {
        "id": "0jRKaSaSOJfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve\n",
        "Bayes, and Bernoulli Naïve Bayes**\n"
      ],
      "metadata": {
        "id": "1gFDB-NF-h2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Naïve Bayes is a supervised machine learning algorithm based on probability and Bayes’ theorem. It works under the assumption that all features are independent of each other. There are different types of Naïve Bayes classifiers, and each one is used for a specific type of data. The three main types are Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes.\n",
        "\n",
        "\n",
        "### 1. Gaussian Naïve Bayes\n",
        "\n",
        "Gaussian Naïve Bayes is used when the features are **continuous numerical values** and follow a normal (Gaussian) distribution.\n",
        "\n",
        "* Assumes data follows a bell-shaped curve\n",
        "* Works well with real-valued data\n",
        "* Commonly used in medical and scientific datasets\n",
        "\n",
        "**Example:**\n",
        "Breast cancer dataset, height, weight, temperature, etc.\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "* Simple and fast\n",
        "* Works well with continuous data\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "* Assumption of normal distribution may not always be correct\n",
        "\n",
        "\n",
        "\n",
        "### 2. Multinomial Naïve Bayes\n",
        "\n",
        "Multinomial Naïve Bayes is mainly used for **discrete count data**.\n",
        "\n",
        "* Works with frequency or count of features\n",
        "* Very popular in text classification\n",
        "* Features represent how many times a word appears\n",
        "\n",
        "**Example:**\n",
        "Spam detection, sentiment analysis, document classification.\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "* Works very well for text data\n",
        "* Efficient and accurate for word counts\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "* Not suitable for continuous data\n",
        "\n",
        "\n",
        "\n",
        "### 3. Bernoulli Naïve Bayes\n",
        "\n",
        "Bernoulli Naïve Bayes is used when features are **binary**, meaning they have only two values such as 0 or 1.\n",
        "\n",
        "* Focuses on whether a feature is present or not\n",
        "* Does not consider frequency, only occurrence\n",
        "\n",
        "**Example:**\n",
        "Whether a word appears in a document or not.\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "* Simple and effective for binary features\n",
        "* Good for small datasets\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "* Loses information about frequency of features\n",
        "\n",
        "\n",
        "\n",
        "### Key Differences (Simple Comparison)\n",
        "\n",
        "* Gaussian NB works with continuous numerical data\n",
        "\n",
        "* Multinomial NB works with count-based data\n",
        "\n",
        "* Bernoulli NB works with binary data\n",
        "\n",
        "* Gaussian NB assumes normal distribution\n",
        "\n",
        "* Multinomial NB uses frequency of features\n",
        "\n",
        "* Bernoulli NB uses presence or absence of features\n",
        "\n",
        "\n",
        "\n",
        "### Use Cases Summary\n",
        "\n",
        "* Use **Gaussian Naïve Bayes** for numerical datasets\n",
        "* Use **Multinomial Naïve Bayes** for text and document data\n",
        "* Use **Bernoulli Naïve Bayes** when features are binary\n",
        "\n",
        "\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Gaussian, Multinomial, and Bernoulli Naïve Bayes classifiers are different versions of the same algorithm designed for different types of data. Choosing the correct type depends on the nature of the dataset and the problem being solved.\n",
        "\n"
      ],
      "metadata": {
        "id": "OYxkui2HNtn8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy.\n",
        "Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from\n",
        "sklearn.datasets.\n",
        "(Include your Python code and output in the code box below.)**\n"
      ],
      "metadata": {
        "id": "CEHbZ-Dq-ka5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Create Gaussian Naive Bayes model\n",
        "model = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"Accuracy of Gaussian Naive Bayes model:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvgQRcanNfVa",
        "outputId": "ade4b1a7-bb9a-4e11-a9ba-8a0f2dd547fc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Gaussian Naive Bayes model: 0.9415204678362573\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eQ_XIug9zr1"
      },
      "outputs": [],
      "source": []
    }
  ]
}